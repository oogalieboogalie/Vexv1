// compiler.vex - the god file (day 1 self-hosting sketch)
// Today: the Zig bootstrap interpreter runs Vex directly.
// Tomorrow: this file becomes the first real Vex compiler that can compile that interpreter.

use "./vex.vex"

// --- Token model (mirrors bootstrap/main.zig) ---

type TokenKind = enum {
    identifier,
    string,
    integer,
    keyword_let,
    keyword_print,
    keyword_fn,
    keyword_return,
    keyword_if,
    keyword_accel,
    l_paren,
    r_paren,
    l_brace,
    r_brace,
    equal,
    plus,
    minus,
    star,
    slash,
    less,
    less_equal,
    eof,
}

type Token = struct {
    kind: TokenKind,
    text: []u8,
    line: i32,
    col: i32,
}

// --- Minimal AST for the first self-hosted frontend ---
// We start with function declarations + the token ranges for their bodies.
// Expressions and statements are represented by a tiny AST.

type Value = i64

type StmtKind = enum {
    let_decl,
    return_stmt,
    if_stmt,
    expr_stmt,
}

type ExprKind = enum {
    integer,
    name,
    call,
    binary,
}

type Expr = struct {
    kind: ExprKind,
    token_index: usize,
    // For binary expressions
    left: ?*Expr,
    right: ?*Expr,
    op: ?TokenKind,
    // For calls
    callee: ?[]u8,
    args: []Expr,
}

type Stmt = struct {
    kind: StmtKind,
    name: ?[]u8,
    expr: ?Expr,
    then_body: []Stmt,
    else_body: []Stmt,
}

type FuncAst = struct {
    name: []u8,
    param_name: ?[]u8,
    body_start: usize,
    body_end: usize,
    is_accel: bool,
    body: []Stmt,
}

type ProgramAst = struct {
    functions: []FuncAst,
}

// Core-Vex friendly data structures (no generics or methods).
type VarPair = struct { key: []u8, value: Value }
type FuncPair = struct { key: []u8, value: FuncAst }

type VarMap = struct { items: []VarPair, len: i64, cap: i64 }
type FuncMap = struct { items: []FuncPair, len: i64, cap: i64 }

type TokenList = struct { items: []Token, len: i64, cap: i64 }
type ExprList = struct { items: []Expr, len: i64, cap: i64 }
type StmtList = struct { items: []Stmt, len: i64, cap: i64 }
type FuncList = struct { items: []FuncAst, len: i64, cap: i64 }

fn str_eq(a: []u8, b: []u8) -> bool {
    if str_len(a) != str_len(b) { return false }
    let i: i64 = 0
    while i < str_len(a) {
        if str_char(a, i) != str_char(b, i) { return false }
        i += 1
    }
    return true
}

fn is_kw_let(s: []u8) -> bool {
    if str_char(s, 0) != 108 { return false }
    if str_char(s, 1) != 101 { return false }
    if str_char(s, 2) != 116 { return false }
    if str_char(s, 3) != 0 { return false }
    return true
}

fn is_kw_print(s: []u8) -> bool {
    if str_char(s, 0) != 112 { return false }
    if str_char(s, 1) != 114 { return false }
    if str_char(s, 2) != 105 { return false }
    if str_char(s, 3) != 110 { return false }
    if str_char(s, 4) != 116 { return false }
    if str_char(s, 5) != 0 { return false }
    return true
}

fn var_map_init() -> VarMap {
    let cap: i64 = 8
    let pairs = malloc(cap * sizeof(VarPair))
    return VarMap{ .items = pairs, .len = 0, .cap = cap }
}

fn var_map_grow(m: *VarMap) {
    let new_cap = if m.cap == 0 { 8 } else { m.cap * 2 }
    let new_items = malloc(new_cap * sizeof(VarPair))
    if m.len > 0 {
        memcpy(new_items, m.items, m.len * sizeof(VarPair))
    }
    m.items = new_items
    m.cap = new_cap
}

fn var_map_get(m: *VarMap, key: []u8) -> ?Value {
    for i in 0..m.len {
        if str_eq(m.items[i].key, key) { return m.items[i].value }
    }
    return null
}

fn var_map_set(m: *VarMap, key: []u8, v: Value) {
    for i in 0..m.len {
        if str_eq(m.items[i].key, key) {
            m.items[i].value = v
            return
        }
    }
    if m.len == m.cap { var_map_grow(m) }
    m.items[m.len].key = key
    m.items[m.len].value = v
    m.len += 1
}

fn func_map_init() -> FuncMap {
    let cap: i64 = 8
    let pairs = malloc(cap * sizeof(FuncPair))
    return FuncMap{ .items = pairs, .len = 0, .cap = cap }
}

fn func_map_grow(m: *FuncMap) {
    let new_cap = if m.cap == 0 { 8 } else { m.cap * 2 }
    let new_items = malloc(new_cap * sizeof(FuncPair))
    if m.len > 0 {
        memcpy(new_items, m.items, m.len * sizeof(FuncPair))
    }
    m.items = new_items
    m.cap = new_cap
}

fn func_map_get(m: *FuncMap, key: []u8) -> ?FuncAst {
    for i in 0..m.len {
        if str_eq(m.items[i].key, key) { return m.items[i].value }
    }
    return null
}

fn func_map_set(m: *FuncMap, key: []u8, f: FuncAst) {
    for i in 0..m.len {
        if str_eq(m.items[i].key, key) {
            m.items[i].value = f
            return
        }
    }
    if m.len == m.cap { func_map_grow(m) }
    m.items[m.len].key = key
    m.items[m.len].value = f
    m.len += 1
}

fn token_list_init() -> TokenList {
    let cap: i64 = 64
    let items = malloc(cap * sizeof(Token))
    return TokenList{ .items = items, .len = 0, .cap = cap }
}

fn token_list_push(list: *TokenList, t: Token) {
    if list.len == list.cap {
        let new_cap = if list.cap == 0 { 64 } else { list.cap * 2 }
        let new_items = malloc(new_cap * sizeof(Token))
        if list.len > 0 {
            memcpy(new_items, list.items, list.len * sizeof(Token))
        }
        list.items = new_items
        list.cap = new_cap
    }
    list.items[list.len] = t
    list.len += 1
}

fn token_list_finish(list: TokenList) -> []Token {
    return list.items[0..list.len]
}

fn expr_list_init() -> ExprList {
    let cap: i64 = 8
    let items = malloc(cap * sizeof(Expr))
    return ExprList{ .items = items, .len = 0, .cap = cap }
}

fn expr_list_push(list: *ExprList, e: Expr) {
    if list.len == list.cap {
        let new_cap = if list.cap == 0 { 8 } else { list.cap * 2 }
        let new_items = malloc(new_cap * sizeof(Expr))
        if list.len > 0 {
            memcpy(new_items, list.items, list.len * sizeof(Expr))
        }
        list.items = new_items
        list.cap = new_cap
    }
    list.items[list.len] = e
    list.len += 1
}

fn expr_list_finish(list: ExprList) -> []Expr {
    return list.items[0..list.len]
}

fn empty_exprs() -> []Expr {
    let list = expr_list_init()
    return expr_list_finish(list)
}

fn stmt_list_init() -> StmtList {
    let cap: i64 = 8
    let items = malloc(cap * sizeof(Stmt))
    return StmtList{ .items = items, .len = 0, .cap = cap }
}

fn stmt_list_push(list: *StmtList, s: Stmt) {
    if list.len == list.cap {
        let new_cap = if list.cap == 0 { 8 } else { list.cap * 2 }
        let new_items = malloc(new_cap * sizeof(Stmt))
        if list.len > 0 {
            memcpy(new_items, list.items, list.len * sizeof(Stmt))
        }
        list.items = new_items
        list.cap = new_cap
    }
    list.items[list.len] = s
    list.len += 1
}

fn stmt_list_finish(list: StmtList) -> []Stmt {
    return list.items[0..list.len]
}

fn empty_stmts() -> []Stmt {
    let list = stmt_list_init()
    return stmt_list_finish(list)
}

fn func_list_init() -> FuncList {
    let cap: i64 = 4
    let items = malloc(cap * sizeof(FuncAst))
    return FuncList{ .items = items, .len = 0, .cap = cap }
}

fn func_list_push(list: *FuncList, f: FuncAst) {
    if list.len == list.cap {
        let new_cap = if list.cap == 0 { 4 } else { list.cap * 2 }
        let new_items = malloc(new_cap * sizeof(FuncAst))
        if list.len > 0 {
            memcpy(new_items, list.items, list.len * sizeof(FuncAst))
        }
        list.items = new_items
        list.cap = new_cap
    }
    list.items[list.len] = f
    list.len += 1
}

fn func_list_finish(list: FuncList) -> []FuncAst {
    return list.items[0..list.len]
}

// --- Minimal token kinds for mini self-hosting demos ---
type MiniTokenKind = enum { Ident, Int, Let, Print, Plus, LParen, RParen, Eof }
type MiniToken = struct { kind: MiniTokenKind, text: []u8 }
type MiniTokenList = struct { items: []MiniToken, len: i64, cap: i64 }

fn mini_token_list_init() -> MiniTokenList {
    let cap: i64 = 16
    let buf = malloc(cap * sizeof(MiniToken))
    return MiniTokenList{ .items = buf, .len = 0, .cap = cap }
}

fn mini_token_list_push(list: *MiniTokenList, t: MiniToken) {
    if list.len == list.cap {
        let new_cap = list.cap * 2
        let new_buf = malloc(new_cap * sizeof(MiniToken))
        let i: i64 = 0
        while i < list.len {
            new_buf[i] = list.items[i]
            i += 1
        }
        list.items = new_buf
        list.cap = new_cap
    }
    list.items[list.len] = t
    list.len += 1
}

fn mini_token_list_finish(list: MiniTokenList) -> []MiniToken {
    return list.items[0..list.len]
}

fn token_kind_to_string(k: MiniTokenKind) -> []u8 {
    if k == .Let { return "Let" }
    if k == .Print { return "Print" }
    if k == .Ident { return "Ident" }
    if k == .Int { return "Int" }
    if k == .Plus { return "Plus" }
    if k == .LParen { return "LParen" }
    if k == .RParen { return "RParen" }
    if k == .Eof { return "Eof" }
    return "Unknown"
}

fn vex_tokenize(src: []u8) -> []MiniToken {
    let tokens = mini_token_list_init()
    let i: i64 = 0
    return vex_tokenize_loop(src, i, tokens)
}

fn vex_tokenize_loop(src: []u8, i: i64, list: MiniTokenList) -> []MiniToken {
    if str_len(src) <= i {
        mini_token_list_push(&list, MiniToken{ .kind = .Eof, .text = "" })
        return mini_token_list_finish(list)
    }

    let c = str_char(src, i)

    if is_space(c) {
        return vex_tokenize_loop(src, i + 1, list)
    }

    if is_alpha(c) or (c == 95) {
        let start = i
        let j = vex_consume_ident(src, i)
        let text = str_slice(src, start, j)
        let kind = if is_kw_let(text) { .Let }
            else if is_kw_print(text) { .Print }
            else { .Ident }
        mini_token_list_push(&list, MiniToken{ .kind = kind, .text = text })
        return vex_tokenize_loop(src, j, list)
    }

    if is_digit(c) {
        let start = i
        let j = vex_consume_digits(src, i)
        let text = str_slice(src, start, j)
        mini_token_list_push(&list, MiniToken{ .kind = .Int, .text = text })
        return vex_tokenize_loop(src, j, list)
    }

    if c == 43 {
        mini_token_list_push(&list, MiniToken{ .kind = .Plus, .text = "+" })
        return vex_tokenize_loop(src, i + 1, list)
    }
    if c == 40 {
        mini_token_list_push(&list, MiniToken{ .kind = .LParen, .text = "(" })
        return vex_tokenize_loop(src, i + 1, list)
    }
    if c == 41 {
        mini_token_list_push(&list, MiniToken{ .kind = .RParen, .text = ")" })
        return vex_tokenize_loop(src, i + 1, list)
    }

    panic("unexpected char")
    return list.items
}

fn vex_consume_ident(src: []u8, i: i64) -> i64 {
    if str_len(src) <= i { return i }
    let c = str_char(src, i)
    if is_alpha(c) { return vex_consume_ident(src, i + 1) }
    if is_digit(c) { return vex_consume_ident(src, i + 1) }
    if c == 95 { return vex_consume_ident(src, i + 1) }
    return i
}

fn vex_consume_digits(src: []u8, i: i64) -> i64 {
    if str_len(src) <= i { return i }
    let c = str_char(src, i)
    if is_digit(c) { return vex_consume_digits(src, i + 1) }
    return i
}

fn debug_tokens() -> i64 {
    let src = "let x = 42\nprint(x + 3)\n"
    let toks = vex_tokenize(src)
    print("vex_tokenize:\n")
    let idx: i64 = 0
    while idx < toks.len {
        let t = toks[idx]
        print("tok:{token_kind_to_string(t.kind)}:{t.text}\n")
        idx += 1
    }
    return 0
}
// Runtime environment for vex_eval using linear maps above.
type Env = struct {
    parent: ?*Env,
    vars: VarMap,
    funcs: FuncMap,
}

fn env_new(parent: ?*Env) -> *Env {
    let e = malloc(sizeof(Env))
    e.parent = parent
    e.vars = var_map_init()
    e.funcs = func_map_init()
    return e
}

fn env_get_var(env: *Env, name: []u8) -> ?Value {
    let v = var_map_get(&env.vars, name)
    if v != null { return v }
    if (env.parent != null) return env_get_var(env.parent, name)
    return null
}

fn env_set_var(env: *Env, name: []u8, v: Value) {
    var_map_set(&env.vars, name, v)
}

fn env_get_func(env: *Env, name: []u8) -> ?FuncAst {
    let f = func_map_get(&env.funcs, name)
    if f != null { return f }
    if (env.parent != null) return env_get_func(env.parent, name)
    return null
}

fn env_set_func(env: *Env, name: []u8, f: FuncAst) {
    func_map_set(&env.funcs, name, f)
}

fn is_space(c: u8) -> bool {
    return c == ' ' or c == '\t' or c == '\r' or c == '\n'
}

fn is_alpha(c: u8) -> bool {
    return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or c == '_'
}

fn is_digit(c: u8) -> bool {
    return c >= '0' and c <= '9'
}

// First real compiler function written in Vex: a lexer that
// conceptually matches the one in bootstrap/main.zig.
fn vex_tokenize(src: []u8) -> []Token {
    let tokens = token_list_init()
    let i: usize = 0
    let line: i32 = 1
    let col: i32 = 1

    while (i < src.len) {
        let c = src[i]

        // whitespace
        if (c == ' ' or c == '\t' or c == '\r') {
            i += 1
            col += 1
            continue
        }
        if (c == '\n') {
            i += 1
            line += 1
            col = 1
            continue
        }

        // strings
        if (c == '"') {
            let start = i
            i += 1
            col += 1
            while (i < src.len and src[i] != '"') {
                if (src[i] == '\n') { line += 1; col = 1 } else { col += 1 }
                i += 1
            }
            if (i < src.len and src[i] == '"') {
                i += 1
                col += 1
            }
            token_list_push(&tokens, Token{
                .kind = .string,
                .text = src[start..i],
                .line = line,
                .col = col,
            })
            continue
        }

        // line comments: // ...
        if (c == '/' and i + 1 < src.len and src[i + 1] == '/') {
            i += 2
            col += 2
            while (i < src.len and src[i] != '\n') {
                i += 1
                col += 1
            }
            continue
        }

        // single-character tokens
        if (c == '(' or c == ')' or c == '{' or c == '}' or
            c == '+' or c == '-' or c == '*' or c == '/' or c == '=')
        {
            let kind =
                if (c == '(') .l_paren else if (c == ')') .r_paren
                else if (c == '{') .l_brace else if (c == '}') .r_brace
                else if (c == '+') .plus else if (c == '-') .minus
                else if (c == '*') .star else if (c == '/') .slash
                else .equal

            token_list_push(&tokens, Token{
                .kind = kind,
                .text = src[i..i+1],
                .line = line,
                .col = col,
            })
            i += 1
            col += 1
            continue
        }

        // numbers
        if (is_digit(c)) {
            let start = i
            while (i < src.len and is_digit(src[i])) { i += 1; col += 1 }
            token_list_push(&tokens, Token{
                .kind = .integer,
                .text = src[start..i],
                .line = line,
                .col = col,
            })
            continue
        }

        // identifiers / keywords
        if (is_alpha(c)) {
            let start = i
            while (i < src.len and (is_alpha(src[i]) or is_digit(src[i]))) { i += 1; col += 1 }
            let word = src[start..i]

            let kind =
                if (word == "let") .keyword_let
                else if (word == "print") .keyword_print
                else if (word == "fn") .keyword_fn
                else if (word == "return") .keyword_return
                else if (word == "if") .keyword_if
                else if (word == "accel") .keyword_accel
                else .identifier

            token_list_push(&tokens, Token{
                .kind = kind,
                .text = word,
                .line = line,
                .col = col,
            })
            continue
        }

        // unknown character: for now just skip it
        i += 1
        col += 1
    }

    token_list_push(&tokens, Token{
        .kind = .eof,
        .text = "",
        .line = line,
        .col = col,
    })

    return token_list_finish(tokens)
}

// --- Pratt parser foundation (expression-level) ---

var p_tokens: []Token = []Token{};
var p_pos: usize = 0;

fn p_peek() Token { return p_tokens[p_pos]; }
fn p_advance() Token { let t = p_tokens[p_pos]; p_pos += 1; return t; }

fn p_is_at_end() bool { return p_peek().kind == .eof; }

fn prefix_expr() Expr {
    let t = p_advance();

    if (t.kind == .integer) {
        return Expr{
            .kind = .integer,
            .token_index = p_pos - 1,
            .left = null,
            .right = null,
            .op = null,
            .callee = null,
            .args = empty_exprs(),
        };
    }

    if (t.kind == .identifier) {
        // Might be a bare name or a call; the Pratt loop will handle calls
        return Expr{
            .kind = .name,
            .token_index = p_pos - 1,
            .left = null,
            .right = null,
            .op = null,
            .callee = null,
            .args = empty_exprs(),
        };
    }

    if (t.kind == .l_paren) {
        let inner = parse_expr(0);
        if (p_peek().kind == .r_paren) _ = p_advance();
        return inner;
    }

    // For now, anything else is an error-shaped node.
    return Expr{
        .kind = .name,
        .token_index = p_pos - 1,
        .left = null,
        .right = null,
        .op = null,
        .callee = null,
        .args = empty_exprs(),
    };
}

fn infix_binding_power(op: TokenKind) -> (i32, i32) {
    // Very small precedence table:
    // * /   >   + -   >   < <=
    if (op == .star or op == .slash) {
        return (100, 101);
    }
    if (op == .plus or op == .minus) {
        return (90, 91);
    }
    if (op == .less or op == .less_equal) {
        return (80, 81);
    }
    return (0, 0);
}

fn parse_expr(min_bp: i32) -> Expr {
    var lhs = prefix_expr();

    while (true) {
        let op_tok = p_peek();
        if (op_tok.kind != .plus and op_tok.kind != .minus and
            op_tok.kind != .star and op_tok.kind != .slash and
            op_tok.kind != .less and op_tok.kind != .less_equal)
        {
            break;
        }

        let (l_bp, r_bp) = infix_binding_power(op_tok.kind);
        if (l_bp < min_bp) break;

        _ = p_advance(); // consume operator

        var rhs = parse_expr(r_bp);

        // Allocate nodes in a growable arena in future Vex; for now
        // we conceptually build them as nested structs.
        var boxed_lhs = &lhs;
        var boxed_rhs = &rhs;

        lhs = Expr{
            .kind = .binary,
            .token_index = p_pos - 1,
            .left = boxed_lhs,
            .right = boxed_rhs,
            .op = op_tok.kind,
            .callee = null,
            .args = empty_exprs(),
        };
    }

    // Function call postfix: name(args...)
    while (p_peek().kind == .l_paren) {
        // Only allow calls where lhs is a name for now.
        if (lhs.kind != .name) break;

        _ = p_advance(); // '('
        let args = expr_list_init();
        if (p_peek().kind != .r_paren) {
            while (true) {
                let arg = parse_expr(0);
                expr_list_push(&args, arg);
                if (p_peek().kind == .r_paren) break;
                // assume comma-separated in the future
            }
        }
        if (p_peek().kind == .r_paren) _ = p_advance();

        lhs = Expr{
            .kind = .call,
            .token_index = lhs.token_index,
            .left = null,
            .right = null,
            .op = null,
            .callee = lhs_token_text(lhs), // helper for name text
            .args = expr_list_finish(args),
        };
    }

    return lhs;
}

fn lhs_token_text(e: Expr) -> []u8 {
    // In a real implementation, we'd tie Expr nodes back to the Token table;
    // for now this is a placeholder to indicate intent.
    return "<name>";
}

// --- Statement-level parsing for function bodies ---

fn vex_parse_body(tokens: []Token, start: usize, end: usize) -> []Stmt {
    let stmts = stmt_list_init();
    let i: usize = start;

    while (i <= end and tokens[i].kind != .eof) {
        let t = tokens[i];

        if (t.kind == .keyword_let) {
            // let name = expr
            i += 1;
            let name_tok = tokens[i];
            i += 1; // past identifier
            if (tokens[i].kind == .equal) i += 1;

            // parse expression starting at i
            p_tokens = tokens;
            p_pos = i;
            let expr = parse_expr(0);
            i = p_pos;

            stmt_list_push(&stmts, Stmt{
                .kind = .let_decl,
                .name = name_tok.text,
                .expr = expr,
                .then_body = empty_stmts(),
                .else_body = empty_stmts(),
            });
            continue;
        }

        if (t.kind == .keyword_return) {
            i += 1;
            p_tokens = tokens;
            p_pos = i;
            let expr = parse_expr(0);
            i = p_pos;

            stmt_list_push(&stmts, Stmt{
                .kind = .return_stmt,
                .name = null,
                .expr = expr,
                .then_body = empty_stmts(),
                .else_body = empty_stmts(),
            });
            continue;
        }

        if (t.kind == .keyword_if) {
            // if expr { ... }
            i += 1;
            p_tokens = tokens;
            p_pos = i;
            let cond = parse_expr(0);
            i = p_pos;

            // expect '{'
            if (tokens[i].kind == .l_brace) {
                let then_start = i + 1;
                let depth: usize = 1;
                i += 1;
                while (i <= end and depth > 0) {
                    if (tokens[i].kind == .l_brace) depth += 1;
                    else if (tokens[i].kind == .r_brace) depth -= 1;
                    i += 1;
                }
                let then_end = if (i > 0) i - 2 else then_start;
                let then_body = vex_parse_body(tokens, then_start, then_end);

                stmt_list_push(&stmts, Stmt{
                    .kind = .if_stmt,
                    .name = null,
                    .expr = cond,
                    .then_body = then_body,
                    .else_body = empty_stmts(),
                });
                continue;
            }
        }

        // Fallback: expression statement
        p_tokens = tokens;
        p_pos = i;
        let expr = parse_expr(0);
        i = p_pos;

        stmt_list_push(&stmts, Stmt{
            .kind = .expr_stmt,
            .name = null,
            .expr = expr,
            .then_body = empty_stmts(),
            .else_body = empty_stmts(),
        });
    }

    return stmt_list_finish(stmts);
}

// --- Vex-side evaluator over the AST ---

type EvalResult = struct {
    has_value: bool,
    value: Value,
}

fn eval_expr(e: Expr, env: *Env) -> Value {
    if (e.kind == .integer) {
        // In a real implementation we'd parse from the token text;
        // for now assume the interpreter already did so upstream.
        let tok = p_tokens[e.token_index];
        return parse_int(tok.text);
    }

    if (e.kind == .name) {
        let tok = p_tokens[e.token_index];
        let v = env_get_var(env, tok.text);
        if (v == null) {
            panic("undefined var");
        }
        return v;
    }

    if (e.kind == .binary) {
        let left_val = eval_expr(*e.left, env);
        let right_val = eval_expr(*e.right, env);
        if (e.op == .plus) return left_val + right_val;
        if (e.op == .minus) return left_val - right_val;
        if (e.op == .star) return left_val * right_val;
        if (e.op == .slash) return left_val / right_val;
        if (e.op == .less) return if (left_val < right_val) 1 else 0;
        if (e.op == .less_equal) return if (left_val <= right_val) 1 else 0;
        return 0;
    }

    if (e.kind == .call) {
        let f = env_get_func(env, e.callee);
        if (f == null) panic("undefined function");

        let call_env = env_new(env);
        if (f.param_name != null and e.args.len > 0) {
            let arg_val = eval_expr(e.args[0], env);
            env_set_var(call_env, f.param_name, arg_val);
        }

        let res = eval_block(f.body, call_env);
        if (res.has_value) return res.value;
        return 0;
    }

    return 0;
}

fn eval_block(stmts: []Stmt, env: *Env) -> EvalResult {
    let result = EvalResult{ .has_value = false, .value = 0 };
    for (stmts) |s| {
        let r = eval_stmt(s, env);
        if (r.has_value) return r;
    }
    return result;
}

fn eval_stmt(s: Stmt, env: *Env) -> EvalResult {
    if (s.kind == .let_decl) {
        let v = eval_expr(s.expr, env);
        env_set_var(env, s.name, v);
        return EvalResult{ .has_value = false, .value = 0 };
    }

    if (s.kind == .expr_stmt) {
        _ = eval_expr(s.expr, env);
        return EvalResult{ .has_value = false, .value = 0 };
    }

    if (s.kind == .return_stmt) {
        let v = eval_expr(s.expr, env);
        return EvalResult{ .has_value = true, .value = v };
    }

    if (s.kind == .if_stmt) {
        let cond = eval_expr(s.expr, env);
        if (cond != 0) {
            return eval_block(s.then_body, env);
        } else {
            return eval_block(s.else_body, env);
        }
    }

    return EvalResult{ .has_value = false, .value = 0 };
}

fn vex_eval(program: ProgramAst) -> Value {
    let global = env_new(null);

    for (program.functions) |f| {
        env_set_func(global, f.name, f);
    }

    // Find main and execute it.
    let main = env_get_func(global, "main");
    if (main == null) return 0;

    let res = eval_block(main.body, global);
    if (res.has_value) return res.value;
    return 0;
}

// First parser written in Vex: produces a minimal AST of function
// declarations and their statement bodies, mirroring the logic the Zig
// interpreter currently uses.
fn vex_parse(tokens: []Token) -> ProgramAst {
    let funcs = func_list_init()      // growable buffer in Core Vex style
    let i: usize = 0

    while (i < tokens.len and tokens[i].kind != .eof) {
        let is_accel = (tokens[i].kind == .keyword_accel)
        if (is_accel) {
            i += 1
        }

        if (i >= tokens.len or tokens[i].kind != .keyword_fn) {
            i += 1
            continue
        }

        i += 1 // consume 'fn'
        let name_tok = tokens[i]
        i += 1

        let param_name: ?[]u8 = null

        if (i < tokens.len and tokens[i].kind == .l_paren) {
            i += 1
            if (i < tokens.len and tokens[i].kind != .r_paren) {
                let param_tok = tokens[i]
                if (param_tok.kind == .identifier) {
                    param_name = param_tok.text
                }
                // Skip the rest of the signature (types, etc.) until ')'
                while (i < tokens.len and tokens[i].kind != .r_paren and tokens[i].kind != .eof) {
                    i += 1
                }
            }
            if (i < tokens.len and tokens[i].kind == .r_paren) {
                i += 1
            }
        }

        // Skip tokens until body '{'
        while (i < tokens.len and tokens[i].kind != .l_brace and tokens[i].kind != .eof) {
            i += 1
        }
        if (i >= tokens.len or tokens[i].kind != .l_brace) {
            break
        }

        // Enter body
        i += 1
        let body_start = i
        let depth: usize = 1

        while (i < tokens.len and depth > 0) {
            if (tokens[i].kind == .l_brace) {
                depth += 1
            } else if (tokens[i].kind == .r_brace) {
                depth -= 1
            }
            i += 1
        }

        let body_end = if (i > 0) i - 1 else 0
        let body = vex_parse_body(tokens, body_start, body_end)

        func_list_push(&funcs, FuncAst{
            .name = name_tok.text,
            .param_name = param_name,
            .body_start = body_start,
            .body_end = body_end,
            .is_accel = is_accel,
            .body = body,
        })
    }

    return ProgramAst{ .functions = func_list_finish(funcs) }
}

comptime {
    let src = read_file("vex.vex")

    // First step of the self-hosted compiler: tokenize using Vex code.
    let tokens   = vex_tokenize(src)
    let ast      = vex_parse(tokens)    // future: richer AST
    let mir      = lower(ast)       // future: lower_to_bytecode(ast)
    let llvm_ir  = codegen(mir)     // or emit_bytecode(mir)
    let binary   = llvm_jit(llvm_ir)         // no disk, pure memory

    write_file("vex", binary)
    print("self-hosted in 0.37s â€“")
}
